{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00c64ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d18c72d",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13d71f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Loading the .env file\n",
    "load_dotenv()\n",
    "ACCESS_TOKEN = os.getenv(\"GITHUB_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f6936",
   "metadata": {},
   "source": [
    "* By default, pull requests are considered issues as well, here we chose to exclude them from data by setting include_prs=False\n",
    "* Setting state = \"all\" means we will load both open and closed issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b3fb25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitHubIssuesLoader\n",
    "\n",
    "loader = GitHubIssuesLoader(repo=\"huggingface/peft\", access_token=ACCESS_TOKEN, include_prs=False, state=\"all\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb424f",
   "metadata": {},
   "source": [
    "* Content of individual GitHub issues may be longer than what an embedding model can take as input.\n",
    "* If we want to embed all of the available content, we need to chunk the documents into   appropriately sized pieces.\n",
    "* The recommended splitter for generic text is the RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a8db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
    "chunked_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293eaeec",
   "metadata": {},
   "source": [
    "# Create the embeddings + retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c041ffa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=hf_token)  # Authenticate with Hugging Face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cef1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": device}\n",
    ")\n",
    "db = FAISS.from_documents(chunked_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c1ef77",
   "metadata": {},
   "source": [
    "* search_type=\"similarity\" means we want to perform similarity search between the query and documents\n",
    "* search_kwargs={'k': 4} instructs the retriever to return top 4 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f489e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb96e06",
   "metadata": {},
   "source": [
    "# Load quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b66aa92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc44e2ab8c545dd849364af22be4a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad674eae",
   "metadata": {},
   "source": [
    "# Setup the LLM chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4a723c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b329143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,         \n",
    "    return_full_text=True,              # return_full_text=False,\n",
    "    max_new_tokens=400\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9f559d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "Answer the question based on your knowledge. Use the following context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "</s>\n",
    "<|user|>\n",
    "{question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\n",
    " \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "479bebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0890d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97084cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do you combine multiple adapters?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a1e9123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|system|>\n",
      "Answer the question based on your knowledge. Use the following context to help:\n",
      "\n",
      "\n",
      "\n",
      "</s>\n",
      "<|user|>\n",
      "How do you combine multiple adapters?\n",
      "</s>\n",
      "<|assistant|>\n",
      "\n",
      "  To combine multiple adapters, you need to ensure that they are compatible with each other and with the devices you want to connect. Here's how you can do it:\n",
      "\n",
      "1. Determine which adapter(s) you need: Consider the types of connections you need to make. For example, if you want to connect a USB device to an Ethernet network, you'll need a USB-to-Ethernet adapter. If you want to connect a VGA monitor to an HDMI input, you'll need a VGA-to-HDMI adapter.\n",
      "\n",
      "2. Check compatibility: Make sure that the adapters you choose are compatible with each other and with the devices you want to connect. For example, if you have a USB-to-Ethernet adapter and a USB hub, you may need to check whether the hub is compatible with the adapter.\n",
      "\n",
      "3. Connect the adapters: Plug one end of the first adapter into the device you want to connect (such as a laptop or a monitor), and then plug the second adapter into the first one. Continue connecting adapters until all the necessary connections are made.\n",
      "\n",
      "4. Test the connection: Once you've connected all the adapters, test the connection to make sure everything is working properly. This may involve turning on the device you're trying to connect, checking for any error messages, or simply verifying that the device is functioning as expected.\n",
      "\n",
      "5. Troubleshoot any issues: If you encounter any problems during the setup process, try troubleshooting the issue by checking the connections, making sure all the drivers are installed correctly, and ensuring that the adapters are compatible with each other and with the devices you're trying to connect. You may also need to consult the user manuals for the adapters and devices involved to get more information about how to resolve any issues\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.invoke({\"context\": \"\", \"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12a60660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|system|>\n",
      "Answer the question based on your knowledge. Use the following context to help:\n",
      "\n",
      "[Document(id='6069499a-eaa5-498c-9315-9f07ec95144e', metadata={'url': 'https://github.com/huggingface/peft/issues/1497', 'title': 'Cannot load adapters from Peft.from_pretrained', 'creator': 'dame-cell', 'created_at': '2024-02-21T13:37:31Z', 'comments': 0, 'state': 'closed', 'labels': [], 'assignee': None, 'milestone': None, 'locked': False, 'number': 1497, 'is_pull_request': False}, page_content='### Expected behavior\\r\\n\\r\\nI think I would want the adapters to be downloaded and then I would be able to merge my adapters into one'), Document(id='f303020f-944e-495f-a8ed-ca88e21a3874', metadata={'url': 'https://github.com/huggingface/peft/issues/1802', 'title': 'Issues when switching between multiple adapters LoRAs ', 'creator': 'JhonDan1999', 'created_at': '2024-05-26T19:18:13Z', 'comments': 8, 'state': 'closed', 'labels': [], 'assignee': None, 'milestone': None, 'locked': False, 'number': 1802, 'is_pull_request': False}, page_content='The documentation does not mention the need to perform a merge when switching adapters. Additionally, the methods add_adapter, set_adapter, and enable_adapters do not appear to work\\r\\n\\r\\nPlease provide clarification on how to correctly switch between adapters'), Document(id='8dc51ee8-d6bb-494c-b9b6-3328de965ea7', metadata={'url': 'https://github.com/huggingface/peft/issues/1040', 'title': 'Multiple Adapters merging question', 'creator': 'AlbertoZerbinati', 'created_at': '2023-10-20T10:02:31Z', 'comments': 5, 'state': 'closed', 'labels': [], 'assignee': None, 'milestone': None, 'locked': False, 'number': 1040, 'is_pull_request': False}, page_content=\"```\\r\\n\\r\\nSo I'd like to ask: is this the proper way to merge multiple Lora adapters into the same model? Is there a reference to follow? I didn't find one.\"), Document(id='28493492-2af8-4383-a701-17d93cac4129', metadata={'url': 'https://github.com/huggingface/peft/issues/1434', 'title': 'Running multiple adapters simultaneously, reusing the output from the base model', 'creator': 'sachin-patel-qp', 'created_at': '2024-02-05T11:56:15Z', 'comments': 4, 'state': 'closed', 'labels': [], 'assignee': None, 'milestone': None, 'locked': False, 'number': 1434, 'is_pull_request': False}, page_content=\"I'm new to the concept of adapters and had a question: Is it possible to run multiple adapters simultaneously while reusing the output from the base model?\\r\\n\\r\\nFor instance, consider a base model named Base_model and 2 adapters, Adapter_1 and Adapter_2. \\r\\nCurrently, to obtain outputs from these two different adapters, we need to load and run them separately:\\r\\n\\r\\n1. Base_model --> Adapter_1 ---> Output_1\\r\\n2. Base_model --> Adapter_2 ---> Output_2\")]\n",
      "\n",
      "</s>\n",
      "<|user|>\n",
      "How do you combine multiple adapters?\n",
      "</s>\n",
      "<|assistant|>\n",
      "\n",
      "  Based on the provided context, it seems that combining or merging multiple adapters involves the following steps:\n",
      "\n",
      "  1. Load the base model and the required adapters using `Peft`.\n",
      "  2. Add each adapter to the base model using the `add_adapter()` method.\n",
      "  3. Enable the desired adapters using the `enable_adapters()` method.\n",
      "  4. Set the desired adapter as active using the `set_adapter()` method.\n",
      "  5. Fine-tune the model with the combined adapters using the `fit()` method.\n",
      "\n",
      "Here's an example code snippet:\n",
      "\n",
      "```python\n",
      "from peft import Peft, TaskType\n",
      "from transformers import AutoModelForSequenceClassification\n",
      "\n",
      "# Load the base model and the required adapters\n",
      "base_model = AutoModelForSequenceClassification.from_pretrained(\"your_base_model\")\n",
      "adapter_1 = Peft.from_tf(tf.keras.models.load_model(\"path/to/adapter_1\"))\n",
      "adapter_2 = Peft.from_tf(tf.keras.models.load_model(\"path/to/adapter_2\"))\n",
      "\n",
      "# Add the adapters to the base model\n",
      "base_model.add_adapter(adapter_1)\n",
      "base_model.add_adapter(adapter_2)\n",
      "\n",
      "# Enable the desired adapters\n",
      "base_model.enable_adapters([TaskType.MULTILABEL_CLASSIFICATION])\n",
      "\n",
      "# Set the desired adapter as active\n",
      "base_model.set_adapter(TaskType.MULTILABEL_CLASSIFICATION)\n",
      "\n",
      "# Fine-tune the model with the combined adapters\n",
      "train_data =...\n",
      "val_data =...\n",
      "test_data =...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
